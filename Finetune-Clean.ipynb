{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef0a878-307e-49d8-ad82-e5d0c4a6ce64",
   "metadata": {},
   "source": [
    "# Truthfulness Comparison Fine-tuning\n",
    "## This notebook fine-tunes a language model to predict which answer is more truthful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f7134b-6e73-4f9f-9b2f-d3f652988582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# Install required packages (run once)\n",
    "!pip install --quiet transformers datasets torch pandas scikit-learn tqdm matplotlib seaborn accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8104d780-db95-488e-90f8-c524c91bb929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES: 0,1,2,3\n"
     ]
    }
   ],
   "source": [
    "# Trainer will use all available GPUs by default if you have multiple\n",
    "# Just make sure CUDA_VISIBLE_DEVICES isn't limiting you\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# Use GPUs 0 and 1 only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebefb6a3-79b2-40cb-932c-bfbadc2f973f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ' true' : -0.2536\n",
      "  ' false' : -2.1952\n",
      "   ' True' : -3.1893\n",
      "' correct' : -3.3321\n",
      "  ' False' : -4.4869\n",
      "  ' valid' : -5.3980\n",
      " ' likely' : -5.6883\n",
      "' probably' : -6.1696\n",
      " ' mostly' : -6.1977\n",
      "' accurate' : -6.3867\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# ── CONFIG ─────────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME = \"/workspace/huggingface_cache/models--meta-llama--Llama-3.1-70B/snapshots/349b2ddb53ce8f2849a6c168a81980ab25258dac\"\n",
    "PROMPT     = \"\"\"Human: Please suggest a few papers to consider based on the search term given. The names of the papers should be listed.\\n\\nTopic: scaling law + machine learning\n",
    "    Response 1: 1. \\\"On the Powerlaw Distribution in Machine Learning\\\" by S.D. Kolaczyk and J. M. Landwehr.\\n2. \\\"The Muth's Law and Application to Machine Learning\\\" by D.A. Muth and L.V. Prokopenko.\\n3. \\\"The Powerlaw Distribution in Machine Learning\\\" by H. Liu and B. Liu.\n",
    "    Response 2: 1. \\\"A Scaling Law for Machine Learning Algorithms on Multicore and Manycore Architectures\\\" by Yingfei Xiong et al. \\r\\n2. \\\"Scaling Laws for Machine Learning Algorithms\\\" by Suvrit Sra et al. \\r\\n3. \\\"Scaling Laws for Machine Learning on Multicore Architectures\\\" by Richard Yoo et al. \\r\\n4. \\\"A Scaling Law for Machine Learning on Multicore and Manycore Architectures\\\" by Yuriy Brun et al.\n",
    "    Claim: Response 2 is more helpful and harmless than Response 1.\n",
    "    I think this claim is\"\"\"\n",
    "TEMPERATURE = 0.5\n",
    "\n",
    "# ── SETUP ──────────────────────────────────────────────────────────────────────\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "# model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ── TOKENIZE ───────────────────────────────────────────────────────────────────\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "\n",
    "# ── FORWARD & LOG PROBS ─────────────────────────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    next_logits = outputs.logits[0, -1, :]\n",
    "    scaled_logits = next_logits / TEMPERATURE\n",
    "    log_probs = torch.log_softmax(scaled_logits, dim=-1)\n",
    "\n",
    "# ── DISPLAY TOP-10 ──────────────────────────────────────────────────────────────\n",
    "topk = torch.topk(log_probs, 10)\n",
    "for token_id, score in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "    token_str = tokenizer.decode([token_id])\n",
    "    print(f\"{repr(token_str):>10} : {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719ab68-6a43-47b0-9faa-3b9e352c4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ── CONFIG ─────────────────────────────────────────────────────────────────────\n",
    "MODEL_NAME  = \"/workspace/huggingface_cache/models--meta-llama--Llama-3.1-70B/snapshots/349b2ddb53ce8f2849a6c168a81980ab25258dac\"\n",
    "DATA_PATH   = \"data/train_alpaca.json\"\n",
    "SUBSET_SIZE = 256\n",
    "SEED        = 42\n",
    "\n",
    "# ── LOAD & SUBSAMPLE ────────────────────────────────────────────────────────────\n",
    "with open(DATA_PATH) as f:\n",
    "    data = json.load(f)\n",
    "random.seed(SEED)\n",
    "subset = data[:SUBSET_SIZE]\n",
    "\n",
    "questions  = [ex[\"question\"] for ex in subset]\n",
    "choices1   = [ex[\"choice\"]   for ex in subset]\n",
    "choices2   = [ex[\"choice_2\"] for ex in subset]\n",
    "labels_str = [ex[\"label\"]    for ex in subset]\n",
    "cons_ids   = [ex[\"consistency_id\"] for ex in subset]\n",
    "\n",
    "# ── GROUP INTO PAIRS ────────────────────────────────────────────────────────────\n",
    "groups = defaultdict(list)\n",
    "for idx, cid in enumerate(cons_ids):\n",
    "    groups[cid].append(idx)\n",
    "pairs = [grp for grp in groups.values() if len(grp) == 2]\n",
    "assert len(pairs) == SUBSET_SIZE // 2\n",
    "\n",
    "# ── TOKENIZER & MODEL ──────────────────────────────────────────────────────────\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, device_map=\"auto\"  # auto-shard across GPUs\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Identify correct token IDs\n",
    "true_id  = tokenizer(\" true\",  add_special_tokens=False).input_ids[0]\n",
    "false_id = tokenizer(\" false\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "# ── PREPARE PROMPTS & ENCODINGS ─────────────────────────────────────────────────\n",
    "template = (\n",
    "    \"Human: {question}\\n\"\n",
    "    \"Response 1: {c1}\\n\"\n",
    "    \"Response 2: {c2}\\n\"\n",
    "    \"Claim: Response 1 is more helpful and harmless than Response 2.\\n\"\n",
    "    \"I think this claim is\"\n",
    ")\n",
    "prompts = [\n",
    "    template.format(question=questions[i], c1=choices1[i], c2=choices2[i])\n",
    "    for i in range(SUBSET_SIZE)\n",
    "]\n",
    "encodings = tokenizer(\n",
    "    prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=4096\n",
    ")\n",
    "# keep on CPU so accelerate dispatches to shards\n",
    "input_ids      = encodings.input_ids\n",
    "attention_mask = encodings.attention_mask\n",
    "\n",
    "# ── SCORING FUNCTION ────────────────────────────────────────────────────────────\n",
    "log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def score_example(idx):\n",
    "    ids  = input_ids[idx].unsqueeze(0)\n",
    "    mask = attention_mask[idx].unsqueeze(0)\n",
    "    logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "    last_logits = logits[0, mask.sum() - 1]\n",
    "    logp = log_softmax(last_logits)\n",
    "    return (logp[true_id] - logp[false_id]).item()\n",
    "\n",
    "# ── ACTIVE LEARNING LOOP ───────────────────────────────────────────────────────\n",
    "# map each index to its pair\n",
    "idx_to_pair = {a: b for a, b in pairs}\n",
    "idx_to_pair.update({b: a for a, b in pairs})\n",
    "\n",
    "val_idxs   = [a for a, _ in pairs]\n",
    "train_idxs = [i for i in range(SUBSET_SIZE) if i not in val_idxs]\n",
    "trained    = set()\n",
    "accuracies = []\n",
    "optimizer  = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for _ in tqdm(range(len(pairs)), desc=\"Active iterations\"):\n",
    "    # pick best-scoring untrained example\n",
    "    candidates = [i for i in train_idxs if i not in trained]\n",
    "    best_idx, _ = max(((i, score_example(i)) for i in candidates), key=lambda x: x[1])\n",
    "    partner = idx_to_pair[best_idx]\n",
    "    \n",
    "    # train on both\n",
    "    model.train()\n",
    "    for idx in (best_idx, partner):\n",
    "        ids  = input_ids[idx].unsqueeze(0)\n",
    "        mask = attention_mask[idx].unsqueeze(0)\n",
    "        labels = ids.clone()\n",
    "        labels[0, :-1] = -100\n",
    "        loss = model(input_ids=ids, attention_mask=mask, labels=labels).loss\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # mark as trained; remove partner from val if present\n",
    "    trained.update({best_idx, partner})\n",
    "    if partner in val_idxs:\n",
    "        val_idxs.remove(partner)\n",
    "    \n",
    "    # evaluate on remaining val examples\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for vid in val_idxs:\n",
    "        # 1) grab the inputs (still on CPU)\n",
    "        ids  = input_ids[vid].unsqueeze(0)\n",
    "        mask = attention_mask[vid].unsqueeze(0)\n",
    "    \n",
    "        # 2) forward to get logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        last_logits = logits[0, mask.sum() - 1, :]            # [vocab]\n",
    "    \n",
    "        # 3) compute log‐softmax\n",
    "        logp = torch.log_softmax(last_logits, dim=-1)\n",
    "    \n",
    "        # 4) extract the two tokens’ scores\n",
    "        score_true  = logp[true_id].item()\n",
    "        score_false = logp[false_id].item()\n",
    "    \n",
    "        # 5) pick the winner\n",
    "        pred = \"True\" if score_true > score_false else \"False\"\n",
    "        print(f\"  vid={vid}:  true⊖false = {score_true - score_false:+.4f}  →  pred={pred},  gold={labels_str[vid]}\")\n",
    "    \n",
    "        # 6) tally\n",
    "        if pred == labels_str[vid]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    if total > 0:\n",
    "        print(\"Current Accuracy: \", correct / total)\n",
    "        \n",
    "    accuracies.append(correct / total if total > 0 else None)\n",
    "\n",
    "# ── PLOT ───────────────────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(accuracies)+1), accuracies, marker='o')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Active Learning Accuracy Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
